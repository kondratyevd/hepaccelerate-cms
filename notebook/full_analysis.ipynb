{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMM_PATH = \"/storage/user/jpata/hmm/dev/hepaccelerate-cms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import os\n",
    "import dask\n",
    "import distributed\n",
    "import glob\n",
    "import shutil\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "from dask.distributed import get_worker, wait, progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibResource():\n",
    "    _args = None\n",
    "    _analysis_parameters = None\n",
    "    _analysis_corrections = None\n",
    "    \n",
    "    def get_resource(self):\n",
    "        if self._args is None:\n",
    "            self._args, self._analysis_parameters, self._analysis_corrections = initialize_worker()\n",
    "        return self._args, self._analysis_parameters, self._analysis_corrections\n",
    "\n",
    "def initialize_worker():\n",
    "    \n",
    "    import sys\n",
    "    sys.path += [HMM_PATH + \"/hepaccelerate\", HMM_PATH + \"/coffea\", HMM_PATH, HMM_PATH + \"/tests/hmm\"]\n",
    "\n",
    "    os.environ[\"NUMBA_NUM_THREADS\"] = str(1)\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(1)\n",
    "\n",
    "    import hmumu_utils, hmumu_lib\n",
    "    from analysis_hmumu import AnalysisCorrections\n",
    "    import hepaccelerate\n",
    "    from pars import analysis_parameters\n",
    "\n",
    "    os.chdir(HMM_PATH)\n",
    "    hmumu_utils.NUMPY_LIB, hmumu_utils.ha = hepaccelerate.choose_backend(use_cuda=False)\n",
    "    analysis_parameters[\"baseline\"][\"do_factorized_jec\"] = False\n",
    "    analysis_parameters[\"baseline\"][\"save_dnn_vars\"] = False\n",
    "    args = Namespace()\n",
    "    args.out = \"./out\"\n",
    "    args.cache_location = \"/storage/user/jpata/hmm/cache\"\n",
    "    args.datapath = \"/storage/group/allcit/\"\n",
    "    args.do_fsr = False\n",
    "    args.async_data = False\n",
    "    args.use_cuda = False\n",
    "    args.do_sync = False\n",
    "    args.nthreads = 1\n",
    "    do_tensorflow = False\n",
    "    \n",
    "    analysis_corrections = AnalysisCorrections(args, do_tensorflow)\n",
    "    return args, analysis_parameters, analysis_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args, analysis_parameters, analysis_corrections = initialize_worker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf ./out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pars\n",
    "import hmumu_utils\n",
    "from hmumu_utils import seed_generator, create_dataset_jobfiles\n",
    "from analysis_hmumu import merge_partial_results\n",
    "seed_gen = seed_generator()\n",
    "\n",
    "jobfiles_dataset = {}\n",
    "if os.path.isdir(\"./out/jobfiles\"):\n",
    "    shutil.rmtree(\"./out/jobfiles\")\n",
    "for dataset_name, dataset_era, globpattern, is_mc in pars.datasets:\n",
    "    filenames = glob.glob(args.datapath + globpattern, recursive=True)\n",
    "    if len(filenames) == 0:\n",
    "        print(args.datapath + globpattern)\n",
    "        break\n",
    "    jobfiles_dataset[(dataset_name, dataset_era)] = create_dataset_jobfiles(dataset_name, dataset_era,\n",
    "            filenames, is_mc, 1, args.out, seed_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -csh out/jobfiles/*.json | tail -n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_filename(jobfile_desc):\n",
    "    ret = copy.deepcopy(jobfile_desc)\n",
    "    for ifn in range(len(ret[\"filenames\"])):\n",
    "        ret[\"filenames\"][ifn] = ret[\"filenames\"][ifn].replace(args.datapath, \"\")\n",
    "    return ret\n",
    "\n",
    "jobfiles_to_process = []\n",
    "for k in jobfiles_dataset.keys():\n",
    "    if k[0] == \"data\":\n",
    "        n = 100\n",
    "    else:\n",
    "        n = 5\n",
    "    jobfiles_to_process += jobfiles_dataset[k]\n",
    "\n",
    "jobfiles_to_process = [fix_filename(jf) for jf in jobfiles_to_process]\n",
    "random.shuffle(jobfiles_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath_partial = \"./out/partial_results\"\n",
    "if not os.path.exists(outpath_partial):\n",
    "    os.makedirs(outpath_partial)\n",
    "#hmumu_utils.run_analysis(args, outpath_partial, jobfiles_to_process[:2], analysis_parameters, analysis_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nworkers = 20\n",
    "#cluster = distributed.LocalCluster(n_workers=nworkers, threads_per_worker=1, nanny=None, processes=True, memory_limit=0)\n",
    "#client = distributed.Client(cluster)\n",
    "\n",
    "#submit using condor_submit dask-worker.jdl\n",
    "#singularity exec /storage/user/jpata/gpuservers/singularity/images/cupy.simg dask-scheduler --host 10.3.18.196 --dashboard-address 131.215.207.131:8178\n",
    "#singularity exec /storage/user/jpata/gpuservers/singularity/images/cupy.simg dask-worker --nthreads 1 --nprocs 20 --memory-limit 0 10.3.18.196:8786\n",
    "client = distributed.Client(\"tcp://10.3.18.196:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure we load the calibration stuff only once\n",
    "def get_library():\n",
    "    worker = get_worker()\n",
    "    try:\n",
    "        return worker.library\n",
    "    except AttributeError:\n",
    "        worker.library = LibResource()\n",
    "        return worker.library\n",
    "\n",
    "#Actually run the phsyics computation!\n",
    "def run_on_worker(job_descriptions):\n",
    "    lr = get_library()\n",
    "    _args, _analysis_parameters, _analysis_corrections = lr.get_resource()\n",
    "    import hmumu_utils\n",
    "    return hmumu_utils.run_analysis(_args, outpath_partial, job_descriptions, _analysis_parameters, _analysis_corrections)\n",
    "    \n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arglist = list(chunks(jobfiles_to_process, 1))\n",
    "print(\"Will process {0} jobs\".format(len(arglist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()\n",
    "t0 = time.time()\n",
    "jobs = client.map(run_on_worker, arglist, retries=3)\n",
    "wait(jobs)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "for f in jobs:\n",
    "    if f.status != \"finished\":\n",
    "        print(\"job {0} failed\".format(f))\n",
    "        failed += [f]\n",
    "        \n",
    "if len(failed) == 0:\n",
    "    print(\"All jobs were successful\")\n",
    "else:\n",
    "    raise Exception(\"Some jobs failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nev_total = sum([f.result()[\"nev_total\"] for f in jobs])\n",
    "dt = (t1 - t0)\n",
    "print(\"Processed {0:.2E} events in {1:.1f} seconds, {2:.2E}\".format(nev_total, dt, nev_total/dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -csh out/partial_results/*.pkl | tail -n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_merge = []\n",
    "for dataset_name, dataset_era, globpattern, is_mc in pars.datasets:\n",
    "    res_merge += [client.submit(merge_partial_results, dataset_name, dataset_era, args.out, outpath_partial)]\n",
    "wait(res_merge);\n",
    "for j in res_merge:\n",
    "    assert(j.status == \"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -csh out/results/*.pkl | tail -n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import make_pdf_plot, get_cross_section\n",
    "from pars import cross_sections, categories, combined_categories, process_groups, extra_plot_kwargs\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era = \"2018\"\n",
    "analysis = \"baseline\"\n",
    "\n",
    "res = {}\n",
    "res[\"data\"] = pickle.load(open(args.out + \"/results/data_{0}.pkl\".format(era), \"rb\"))\n",
    "mc_samples_load = set([d[0] for d in pars.datasets if d[1] == era])\n",
    "for mc_samp in mc_samples_load:\n",
    "    res_file_name = args.out + \"/results/{0}_{1}.pkl\".format(mc_samp, era)\n",
    "    if os.path.isfile(res_file_name):\n",
    "        res[mc_samp] = pickle.load(open(res_file_name, \"rb\"))\n",
    "int_lumi = res[\"data\"][\"baseline\"][\"int_lumi\"]\n",
    "\n",
    "genweights = {}\n",
    "weight_xs = {}\n",
    "for mc_samp in res.keys():\n",
    "    if mc_samp != \"data\":\n",
    "        genweights[mc_samp] = res[mc_samp][\"genEventSumw\"]\n",
    "        weight_xs[mc_samp] = get_cross_section(cross_sections, mc_samp, era) * int_lumi / genweights[mc_samp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(res[\"data\"][analysis].keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"hist__dimuon_invmass_z_peak_cat5__eta_jj\"\n",
    "mc_samples = categories[\"z_peak\"][\"datacard_processes\"]\n",
    "combined_mc_samples = combined_categories[\"z_peak\"][\"datacard_processes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histos = {}\n",
    "for sample in mc_samples + [\"data\"]:\n",
    "    histos[sample] = res[sample][analysis][var]\n",
    "hdata = res[\"data\"][analysis][var][\"nominal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = os.path.join(args.out, analysis, \"plots\", era)\n",
    "weight_nominal = \"nominal\"\n",
    "plot_args = [(\n",
    "    histos, hdata, mc_samples, analysis,\n",
    "    var, weight_nominal, weight_xs, int_lumi, outpath, era, process_groups, extra_plot_kwargs.get(var, {}))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pdf_plot(plot_args[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"{0}/png/{1}_{2}_{3}.png\".format(outpath, analysis, var, weight_nominal), width=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
